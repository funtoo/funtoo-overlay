#!/usr/bin/python3

import os
import sys
import random
import asyncio
import aioftp
import async_timeout
import aiodns
import aiohttp
import logging
from hashlib import sha256, sha512
import socket
from concurrent.futures import ThreadPoolExecutor
from utils.google_upload_server import google_upload
from utils.spider_common import *
from datetime import datetime, timedelta
from sqlalchemy.orm import undefer

sys.path.insert(0, os.path.normpath(os.path.join(os.path.realpath(__file__), "../modules")))
from merge.db_core import *

resolver = aiohttp.AsyncResolver(nameservers=['8.8.8.8', '8.8.4.4'], timeout=5, tries=3)

thirdp = {}
with open('/var/git/meta-repo/kits/core-kit/profiles/thirdpartymirrors', 'r') as fd:
	for line in fd.readlines():
		ls = line.split()
		thirdp[ls[0]] = []
		for x in ls[1:]:
			if "fastpull" in x:
				continue
			else:
				thirdp[ls[0]].append(x)

# TODO: only try to download one filename of the same name at a time.

# maximum number of third-party mirrors to consider for download:

max_mirrors = 3
mirror_blacklist = [ "gentooexperimental" ]

def src_uri_process(uri_text, fn):
	# converts \n delimited text of all SRC_URIs for file from ebuild into a list containing:
	# [ mirror_path, [ mirrors ] -- where mirrors[0] + "/" + mirror_path is a valid dl path
	#
	# or string, where string is just a single download path.

	global thirdp
	uris_to_process = uri_text.split("\n")
	uris_to_process = [ "http://distfiles.gentoo.org/distfiles/" + fn ] + uris_to_process
	out_uris = []
	for uri in uris_to_process:
		if len(uri) == 0:
			continue
		if uri.startswith("mirror://"):
			uri = uri[9:]
			mirror_name = uri.split("/")[0]
			mirror_path = "/".join(uri.split("/")[1:])
			if mirror_name not in thirdp:
				print("!!! Error: no third-party mirror defined for %s" % mirror_name)
				continue
			out_mirrors = []
			for my_mirror in thirdp[mirror_name]:
				skip = False
				for bl_entry in mirror_blacklist:
					if my_mirror.find(bl_entry) != -1:
						skip = True
						break
				if skip:
					continue
				out_mirrors.append(my_mirror)
			out_uris.append([mirror_path, out_mirrors[:max_mirrors]])
		elif uri.startswith("http://") or uri.startswith("https://") or uri.startswith("ftp://"):
			out_uris.append(uri)
	return out_uris

def get_sha512(fn):
		with open(fn, "rb") as data:
			my_hash = sha512()
			my_hash.update(data.read())
			return my_hash.hexdigest()

async def ftp_fetch(host, path, outfile, digest_func):
	client = aioftp.Client()
	await client.connect(host)
	await client.login("anonymous", "drobbins@funtoo.org")
	fd = open(outfile, 'wb')
	hash = digest_func()
	if not await client.exists(path):
		return ("ftp_missing", None)
	stream = await client.download_stream(path)
	async for block in stream.iter_by_block(chunk_size):
		sys.stdout.write(".")
		sys.stdout.flush()
		fd.write(block)
		hash.update(block)
	await stream.finish()
	cur_digest = hash.hexdigest()
	await client.quit()
	fd.close()
	return (None, cur_digest)

http_data_timeout = 60

async def http_fetch(url, outfile, digest_func):
	global resolver
	connector = aiohttp.TCPConnector(family=socket.AF_INET,resolver=resolver,verify_ssl=False)
	headers = {}
	fmode = 'wb'
	hash = digest_func()
	if os.path.exists(outfile):
		os.unlink(outfile)
	async with aiohttp.ClientSession(connector=connector) as http_session:
		async with http_session.get(url, headers=headers, timeout=None) as response:
			if response.status != 200:
				return ("http_%s" % response.status, None)
			with open(outfile, fmode) as fd:
				while True:
					#with aiohttp.Timeout(http_data_timeout):
					try:
						chunk = await response.content.read(chunk_size)
						if not chunk:
							break
						else:
							sys.stdout.write(".")
							sys.stdout.flush()
							fd.write(chunk)
							hash.update(chunk)
					except aiohttp.EofStream:
						pass
	cur_digest = hash.hexdigest()
	return (None, cur_digest)


def next_uri(uri_expand):
	for src_uri in uri_expand:
		if type(src_uri) == list:
			for uri in src_uri[1]:
				real_uri = uri
				if not real_uri.endswith("/"):
					real_uri += "/"
				real_uri += src_uri[0]
				yield real_uri
		else:
			yield src_uri

fastpull_count = 0

def fastpull_index(outfile, distfile_final):
	global fastpull_count
	# add to fastpull.
	d1 = distfile_final.rand_id[0]
	d2 = distfile_final.rand_id[1]
	outdir = os.path.join(fastpull_out, d1, d2)
	if not os.path.exists(outdir):
		os.makedirs(outdir)
	fastpull_outfile = os.path.join(outdir, distfile_final.rand_id)
	if os.path.lexists(fastpull_outfile):
		os.unlink(fastpull_outfile)
	os.link(outfile, fastpull_outfile)
	fastpull_count += 1
	return os.path.join(d1, d2, distfile_final.rand_id)

async def keep_getting_files(db, task_num, q):
	timeout = 4800 

	while True:

		# continually grab files....
		d_id = await q.get()

		progress_map[d_id] = "selected"

		with db.get_session() as session:
			# This will attach to our current session
			await asyncio.sleep(0.1)
			d = session.query(db.QueuedDistfile).filter(db.QueuedDistfile.id == d_id).first()
			if d is None:
				print("File %s is none." % d_id)
				# no longer exists, no longer in progress, next file...
				progress_set.remove(d_id)
				continue

			#if not d.mirror:
			#	print("No mirroring set for %s, deleting from queue." % d.filename)
			#	session.delete(d)
			#	session.commit()
			#	progress_set.remove(d_id)
			#	continue

			if d.digest_type == "sha256":
				digest_func = sha256
			else:
				digest_func = sha512

			uris = []
			if d.src_uri is not None:
				uris = src_uri_process(d.src_uri, d.filename)
			if len(uris) == 0:
				print("Error: for file %s, no URIs available; skipping." % d.filename)
				try:
					session.delete(d)
					session.commit()
				except sqlalchemy.exc.InvalidRequestError:
					# already deleted by someone else
					pass
				# move to next file...
				d.last_attempted_on = datetime.utcnow()
				session.add(d)
				session.commit()
				progress_set.remove(d_id)
				continue
		
			filename = d.filename
			outfile = os.path.join("/home/mirror/distfiles/%s/%s" % (task_num, filename))
			os.makedirs(os.path.dirname(outfile))
			mylist = list(next_uri(uris))
			fail_mode = None

			progress_map[d_id] = "dl_check"

			# if we have a sha512, then we can to a pre-download check to see if the file has been grabbed before.
			if d.digest_type == "sha512" and d.digest is not None:
				existing = session.query(db.Distfile).filter(db.Distfile.id == d.digest).first()
				if existing:
					print("%s already downloaded; skipping." % d.filename)
					session.delete(d)
					session.commit()
					# move to next file....
					progress_set.remove(d_id)
					continue
			session.expunge_all()

		# force session close before download by exiting "with"

		last_uri = None

		for real_uri in mylist:

			# iterate through each potential URI for downloading a particular distfile. We'll keep trying until
			# we find one that works.

			# fail_mode will effectively store the last reason why our download failed. We reset it each iteration,
			# which is what we want. If fail_mode is set to something after our big loop exits, we know we have
			# truly failed downloading this distfile.

			print("Trying URI", real_uri)

			progress_map[d_id] = real_uri
			fail_mode = None

			if real_uri.startswith("ftp://"):
				# handle ftp download --
				host_parts = real_uri[6:]
				host = host_parts.split("/")[0]
				path = "/".join(host_parts.split("/")[1:])
				try:
					digest = None
					with async_timeout.timeout(timeout):
						fail_mode, digest = await ftp_fetch(host, path, outfile, digest_func)
				except asyncio.TimeoutError as e:
					fail_mode = "timeout"
					continue
				except socket.gaierror as e:
					fail_mode = "dnsfail"
					continue
				except OSError:
					fail_mode = "refused"
					continue
				except aioftp.errors.StatusCodeError:
					fail_mode = "ftp_code"
					continue
				except Exception as e:
					fail_mode = str(e)
					raise
					print("Download failure:", fail_mode)
					continue
			else:
				# handle http/https download --
				try:
					digest = None
					with async_timeout.timeout(timeout):
						fail_mode, digest = await http_fetch(real_uri, outfile, digest_func)
				except asyncio.TimeoutError as e:
					fail_mode = "timeout"
					continue
				except aiodns.error.DNSError as e:
					fail_mode = "dnsfail"
					continue
				except ValueError as e:
					fail_mode = "bad_url"
					continue
				except aiohttp.errors.ClientOSError as e:
					fail_mode = "refused"
					continue
				except Exception as e:
					fail_mode = str(e)
					raise
					print("Download failure:", fail_mode)
					continue

			del progress_map[d_id]

			if d.digest is None or (digest is not None and digest == d.digest):
				# success! we can record our fine ketchup:

				if d.digest_type == "sha512" and digest is not None:
					my_id = digest
				else:
					try:
						my_id = get_sha512(outfile)
					except FileNotFoundError:
						fail_mode = "notfound"
						continue
					
				# create new session after download completes (successfully or not)
				with db.get_session() as session:

					existing = session.query(db.Distfile).filter(db.Distfile.id == my_id).first()

					if existing is not None:
						if existing.filename == filename:
							print("Downloaded %s, but already exists in our db. Skipping." % d.filename)
							fail_mode = None
							session.delete(d)
							session.commit()
							os.unlink(outfile)
							# done; process next distfile
							break

					d_final = db.Distfile()

					d_final.id = my_id
					d_final.rand_id = ''.join(random.choice('abcdef0123456789') for _ in range(128))
					d_final.filename = d.filename
					d_final.digest_type = d.digest_type
					if d.digest_type != "sha512":
						d_final.alt_digest = digest
					d_final.size = d.size
					d_final.catpkg = d.catpkg
					d_final.kit = d.kit
					d_final.src_uri = d.src_uri
					d_final.mirror = d.mirror
					d_final.last_fetched_on = datetime.utcnow()


					try:
						fastpull_file = fastpull_index(outfile, d_final)
						# add to queue to upload to google:
						loop = asyncio.get_event_loop()
						loop.run_in_executor(thread_exec, google_upload, fastpull_file)

					except FileNotFoundError:
						# something went bad, couldn't find file for indexing.
						fail_mode = "notfound"
						continue

					session.add(d_final)
					session.delete(d)
					session.commit()

					os.unlink(outfile)
					# done; process next distfile
					break
			else:
				fail_mode = "digest"

		if fail_mode:
			# If we tried all SRC_URIs, and still failed, we will end up here, with fail_mode set to something.
			with db.get_session() as session:
				d = session.query(db.QueuedDistfile).filter(db.QueuedDistfile.id == d_id).first()
				if d == None:
					# object no longer exists, so skip this update:
					pass
				else:
					d.last_failure_on = d.last_attempted_on = datetime.utcnow()
					d.failtype = fail_mode
					d.failcount += 1
					session.add(d)
					session.commit()
				print()
				print("Download failure: %s" % d.filename)
				if last_uri:
					print("  Last URI:", last_uri)
				print("  Failure reason: %s" % fail_mode)
				print("  Expected filesize: %s" % d.size)
				outfile = os.path.join("/home/mirror/distfiles/", d.filename)
				if os.path.exists(outfile):
					print("  Partial filesize: %s" % os.path.getsize(outfile))
				print()
		else:
			# we end up here if we are successful. Do successful output.
			sys.stdout.write("^")
			sys.stdout.flush()
		progress_set.remove(d_id)

queue_size = 60
query_size = 60
workr_size = 10

pending_q = asyncio.Queue(maxsize=queue_size)
# set of all QueuedDistfile IDs currently being processed:
progress_set = set()
# dictionary of status info for all QueuedDistfile IDs:
progress_map = {}

async def qsize(q):
	while True:
		print()
		print("Queue size: %s" % q.qsize())
		print("Added to fastpull: %s" % fastpull_count)
		print("In pending queue: %s" % pending_q.qsize())
		print("In progress: %s" % len(list(map(str,progress_set))))
		print("IDs in progress:")
		for my_id in sorted(list(progress_set)):
			print("{:8s}".format(str(my_id)), end="")
			if my_id in progress_map:
				print(progress_map[my_id])
			else:
				print()
		# clean up stale progress_map entries
		to_del = []
		for my_id in progress_map.keys():
			if my_id not in progress_set:
				to_del.append(my_id)
		for my_id in to_del:
			del progress_map[my_id]
		await asyncio.sleep(15)

async def get_more_distfiles(db, q):
	global now
	time_cutoff = datetime.utcnow() - timedelta(hours=24)
	#time_cutoff_hr = datetime.utcnow() - timedelta(hours=2)
	time_cutoff_hr = datetime.utcnow()
	# The asyncio.sleep() calls below not only sleep, they also turn this into a true async function. Otherwise we
	# would not allow other coroutines to run.
	while True:
		with db.get_session() as session:
			results = session.query(db.QueuedDistfile)
			results = results.options(undefer('last_attempted_on'))
			results = results.filter(or_(db.QueuedDistfile.last_attempted_on < time_cutoff, db.QueuedDistfile.last_attempted_on == None))
			#results = results.filter(db.QueuedDistfile.mirror == True)
			results = results.order_by(db.QueuedDistfile.last_attempted_on)
			results = list(results.limit(query_size))
			session.expunge_all()
		# force session to close here
		if len(list(results)) == 0:
			await asyncio.sleep(5)
		else:
			added = 0
			for d in results:
				if d.id not in progress_set:
					await q.put(d.id)
					# track file ids in progress.
					progress_set.add(d.id)
					progress_map[d.id] = "queued"
					added += 1
			if added == 0:
				await asyncio.sleep(0.5)


#import logging
#logging.basicConfig()
#logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)

logging.basicConfig(
	level=logging.INFO,
	format='PID %(process)5s %(name)18s: %(message)s',
	stream=sys.stderr,
)

google_server_status = None
	
chunk_size = 65536
db = FastPullDatabase()
loop = asyncio.get_event_loop()
now = datetime.utcnow()
thread_exec = ThreadPoolExecutor(max_workers=1)
tasks = [
	asyncio.async(get_more_distfiles(db, pending_q)),
	asyncio.async(qsize(pending_q)),
]

for x in range(0,workr_size):
	tasks.append(asyncio.async(keep_getting_files(db, x, pending_q)))

loop.run_until_complete(asyncio.gather(*tasks))
loop.close()

# vim: ts=4 sw=4 noet
