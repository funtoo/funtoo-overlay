#!/usr/bin/python3

import json
import os
import sys
from collections import defaultdict, OrderedDict
from datetime import datetime
import asyncio

sys.path.insert(0, os.path.normpath(os.path.join(os.path.realpath(__file__), "../../modules")))
import merge.merge_utils as mu
from merge.config import config
from merge.async_engine import AsyncEngine


class AsyncMergeAllKits(AsyncEngine):
	_db = None
	
	@property
	def db(self):
		if self._db is None:
			from merge.db_core import FastPullDatabase
			self._db = FastPullDatabase()
		return self._db
	
	def worker_thread(self, **kwargs):
		db = self.db
		with db.get_session() as session:
			f = kwargs["file"]
			existing = session.query(db.Distfile).filter(db.Distfile.id == kwargs["digest"]).first()
			# TODO: maybe it already exists, but under a different filename. If so, we still want to create a distfile entry for it so it can be downloaded...
			
			if existing:
				return
			
			# Don't create multiple queued downloads for the same distfile:
			if session.query(db.QueuedDistfile).filter(db.QueuedDistfile.filename == f).filter(db.QueuedDistfile.size == kwargs["size"]).first() is not None:
				return
			
			# Queue the distfile for downloading...
			
			qd = db.QueuedDistfile()
			qd.filename = f
			qd.catpkg = kwargs["catpkg"]
			qd.kit = kwargs["kit_name"]
			qd.branch = kwargs["kit_branch"]
			qd.src_uri = kwargs["src_uri"]
			qd.size = kwargs["size"]
			qd.mirror = kwargs["restrict"]
			qd.digest_type = kwargs["digest_type"]
			qd.digest = kwargs["digest"]
			qd.priority = 1 if kwargs["bestmatch"] else 0
			session.add(qd)
			session.commit()


# We want to reset 'kitted_catpkgs' at certain points. The 'kit_order' variable below is used to control this, and we
# normally don't need to touch it. 'kitted_order' above tells the code to generate 'prime', then 'shared' (without
# resetting kitted_catpkgs to empty), then the None tells the code to reset kitted_catpkgs, so when 'current' kits are
# generated, they can include from all possible catpkgs. This is done because prime+shared is designed to be our
# primary enterprise-set of Funtoo kits. current+shared is also supported as a more bleeding edge option.

# KIT PREP STEPS. To rebuild kits from scratch, we need to perform some initial actions to initialize an empty git
# repository, as well as some final actions. In the kit_steps dictionary below, indexed by kit, 'pre' dict lists the
# initial actions, and 'post' lists the final actions for the kit. There is also a special top-level key called
# 'regular-kits'. These actions are appended to any kit that is not core-kit or nokit. In addition to 'pre' and 'post'
# steps, there is also a 'copy' step that is not currently used (but is supported by getKitPrepSteps()).

def getKitPrepSteps(repos, kit_dict, gentoo_staging, fixup_repo):
	kit_steps = {
		'core-kit': {'pre': [
			mu.GenerateRepoMetadata("core-kit", aliases=["gentoo"], priority=1000),
			# core-kit has special logic for eclasses -- we want all of them, so that third-party overlays can reference the full set.
			# All other kits use alternate logic (not in kit_steps) to only grab the eclasses they actually use.
			mu.SyncDir(gentoo_staging.root, "eclass"),
		],
			'post': [
				# We copy files into funtoo's profile structure as post-steps because we rely on kit-fixups step to get the initial structure into place
				mu.CopyAndRename("profiles/funtoo/1.0/linux-gnu/arch/x86-64bit/subarch", "profiles/funtoo/1.0/linux-gnu/arch/pure64/subarch",
								 lambda x: os.path.basename(x) + "-pure64"),
				# news items are not included here anymore
				mu.SyncDir(fixup_repo.root, "metadata", exclude=["cache", "md5-cache", "layout.conf"]),
				# add funtoo stuff to thirdpartymirrors
				mu.ThirdPartyMirrors(),
				mu.RunSed(["profiles/base/make.defaults"], ["/^PYTHON_TARGETS=/d", "/^PYTHON_SINGLE_TARGET=/d"]),
			]
		},
		# masters of core-kit for regular kits and nokit ensure that masking settings set in core-kit for catpkgs in other kits are applied
		# to the other kits. Without this, mask settings in core-kit apply to core-kit only.
		'regular-kits': {'pre': [
			mu.GenerateRepoMetadata(kit_dict['name'], masters=["core-kit"], priority=500),
		]
		},
		'all-kits': {'pre': [
			mu.SyncFiles(fixup_repo.root, {
				"COPYRIGHT.txt": "COPYRIGHT.txt",
				"LICENSE.txt": "LICENSE.txt",
			}),
		]
		},
		'nokit': {'pre': [
			mu.GenerateRepoMetadata("nokit", masters=["core-kit"], priority=-2000),
			]
		}
	}
	
	out_pre_steps = []
	out_copy_steps = []
	out_post_steps = []
	
	kd = kit_dict['name']
	if kd in kit_steps:
		if 'pre' in kit_steps[kd]:
			out_pre_steps += kit_steps[kd]['pre']
		if 'post' in kit_steps[kd]:
			out_post_steps += kit_steps[kd]['post']
		if 'copy' in kit_steps[kd]:
			out_copy_steps += kit_steps[kd]['copy']
	
	# a 'regular kit' is not core-kit or nokit -- if we have pre or post steps for them, append these steps:
	if kit_dict['name'] not in ['core-kit', 'nokit'] and 'regular-kits' in kit_steps:
		if 'pre' in kit_steps['regular-kits']:
			out_pre_steps += kit_steps['regular-kits']['pre']
		if 'post' in kit_steps['regular-kits']:
			out_post_steps += kit_steps['regular-kits']['post']
	
	if 'all-kits' in kit_steps:
		if 'pre' in kit_steps['all-kits']:
			out_pre_steps += kit_steps['all-kits']['pre']
		if 'post' in kit_steps['all-kits']:
			out_post_steps += kit_steps['all-kits']['post']
	
	return out_pre_steps, out_copy_steps, out_post_steps


# GET KIT SOURCE INSTANCE. This function returns a list of GitTree objects for each of repositories specified for
# a particular kit's kit_source, in the order that they should be processed (in the order they are defined in
# kit_source_defs, in other words.)

async def getKitSourceInstance(foundation, kit_dict):
	source_name = kit_dict['source']
	
	repos = []
	
	source_defs = foundation.kit_source_defs[source_name]
	
	for source_def in source_defs:
		
		repo_name = source_def['repo']
		repo_branch = source_def['src_branch'] if "src_branch" in source_def else "master"
		repo_sha1 = source_def["src_sha1"] if "src_sha1" in source_def else None
		repo_obj = mu.GitTree
		repo_url = foundation.overlays[repo_name]["url"]
		if "dirname" in foundation.overlays[repo_name]:
			path = foundation.overlays[repo_name]["dirname"]
		else:
			path = repo_name
		repo = repo_obj(repo_name, url=repo_url, root="%s/%s" % (config.source_trees, path), branch=repo_branch, commit_sha1=repo_sha1)
		await repo.initialize()
		repos.append({"name": repo_name, "repo": repo, "overlay_def": foundation.overlays[repo_name]})
	
	return repos


# UPDATE KIT. This function does the heavy lifting of taking a kit specification included in a kit_dict, and
# regenerating it. The kitted_catpkgs argument is a dictionary which is also written to and used to keep track of
# catpkgs copied between runs of updateKit.

async def updateKit(foundation, release, async_engine : AsyncMergeAllKits, kit_dict, prev_kit_dict, cpm_logger, create=False, push=False, now=None, fixup_repo=None):
	# secondary_kit means: we're the second (or third, etc.) xorg-kit or other kit to be processed. The first kind of
	# each kit processed has secondary_kit = False, and later ones have secondary_kit = True. We need special processing
	# to grab any 'orphan' packages that were selected as part of prior kit scans (and thus will not be included in
	# later kits) but were not picked up in our current kit-scan. For example, let's say @depsincat@:virtual/ttf-fonts:
	# media-fonts picks up a funky font in the first xorg-kit scan, but in the second xorg-kit scan, the deps have
	# changed and thus this font isn't selected. Well without special handling, if we are using the second (or later)
	# xorg-kit, funky-font won't exist. We call these guys 'orphans' and need to ensure we include them.
	
	move_maps = mu.get_move_maps(fixup_repo.root + "/move-maps", kit_dict['name'])
	
	secondary_kit = False
	if prev_kit_dict is not None:
		if kit_dict['name'] != prev_kit_dict['name']:
			
			# We are advancing to the next kit. For example, we just processed an xorg-kit and are now processing a python-kit. So we want to apply all our accumulated matches.
			# If we are processing an xorg-kit again, this won't run, which is what we want. We want to keep accumulating catpkg names/matches.
			
			cpm_logger.nextKit()
		
		else:
			secondary_kit = True
	print("Processing kit %s branch %s, secondary kit is %s" % (kit_dict['name'], kit_dict['branch'], repr(secondary_kit)))
	
	# get set of source repos used to grab catpkgs from:

	if "generate" in kit_dict and kit_dict["generate"] is not True:
		# independently-maintained repo. Don't regenerate. Just record all catpkgs in this kit as belonging ot this kit so they don't get into other kits:
		tree = mu.GitTree(kit_dict["name"], kit_dict["branch"], url="https://github.com/funtoo/%s" % kit_dict["name"], root=config.source_trees + "/" + kit_dict["name"])
		await tree.initialize()
		await tree.run([
			mu.RecordAllCatPkgs(tree, cpm_logger)
		])
		return tree.head()
	
	repos = kit_dict["repo_obj"] = await getKitSourceInstance(foundation, kit_dict)
	
	# get a handy variable reference to gentoo_staging:
	gentoo_staging = None
	for x in repos:
		if x["name"] == "gentoo-staging":
			gentoo_staging = x["repo"]
			break
	
	if gentoo_staging is None:
		print("Couldn't find source gentoo staging repo")
	elif gentoo_staging.name != "gentoo-staging":
		print("Gentoo staging mismatch -- name is %s" % gentoo_staging["name"])
	
	# If we have gotten here, we are automatically generating a kit...
	kit_dict['tree'] = tree = mu.GitTree(kit_dict['name'], kit_dict['branch'],
										 url=config.base_url(kit_dict['name']), create=create,
										 root="%s/%s" % (config.dest_trees, kit_dict['name']))
	await tree.initialize()
	if "stability" in kit_dict and kit_dict["stability"] == KitStabilityRating.DEPRECATED:
		# no longer update this kit.
		return tree.head()
	
	# Phase 1: prep the kit
	pre_steps = [
		mu.GitCheckout(kit_dict['branch']),
		mu.CleanTree()
	]
	
	prep_steps = getKitPrepSteps(repos, kit_dict, gentoo_staging, fixup_repo)
	pre_steps += prep_steps[0]
	copy_steps = prep_steps[1]
	post_steps = prep_steps[2]
	
	await tree.run(pre_steps)
	
	# Phase 2: copy core set of ebuilds
	
	# Here we generate our main set of ebuild copy steps, based on the contents of the package-set file for the kit. The logic works as
	# follows. We apply our package-set logic to each repo in succession. If copy ebuilds were actually copied (we detect this by
	# looking for changed catpkg count in our dest_kit,) then we also run additional steps: "copyfiles" and "eclasses". "copyfiles"
	# specifies files like masks to copy over to the dest_kit, and "eclasses" specifies eclasses from the overlay that we need to
	# copy over to the dest_kit. We don't need to specify eclasses that we need from gentoo_staging -- these are automatically detected
	# and copied, but if there are any special eclasses from the overlay then we want to copy these over initially.
	
	copycount = cpm_logger.copycount
	for repo_dict in repos:
		steps = []
		select_clause = "all"
		overlay_def = repo_dict["overlay_def"]
		
		if "select" in overlay_def:
			select_clause = overlay_def["select"]
		
		# If the repo has a "filter" : [ "foo", "bar", "oni" ], then construct a list of repos with those names and put
		# them in filter_repos. We will pass this list of repo objects to InsertEbuilds inside generateKitSteps, and if
		# a catpkg exists in any of these repos, then it will NOT be copied if it is scheduled to be copied for this
		# repo. This is a way we can lock down overlays to not insert any catpkgs that are already defined in gentoo --
		# just add: filter : [ "gentoo-staging" ] and if the catpkg exists in gentoo-staging, it won't get copied. This
		# way we can more safely choose to include all ebuilds from 'potpurri' overlays like faustoo without exposing
		# ourself to too much risk from messing stuff up.
		
		filter_repos = []
		if "filter" in overlay_def:
			for filter_repo_name in overlay_def["filter"]:
				for x in repos:
					if x["name"] == filter_repo_name:
						filter_repos.append(x["repo"])
		
		if kit_dict["name"] == "nokit":
			# grab all remaining ebuilds to put in nokit
			steps += [mu.InsertEbuilds(repo_dict["repo"], select_only=select_clause, move_maps=move_maps, skip=None, replace=False, cpm_logger=cpm_logger)]
		else:
			steps += await mu.generateKitSteps(release, kit_dict['name'], repo_dict["repo"], fixup_repo=fixup_repo, select_only=select_clause,
										 filter_repos=filter_repos, force=overlay_def["force"] if "force" in overlay_def else None,
										 cpm_logger=cpm_logger, move_maps=move_maps, secondary_kit=secondary_kit)
		await tree.run(steps)
		if copycount != cpm_logger.copycount:
			# this means some catpkgs were installed from the repo we are currently processing. This means we also want to execute
			# 'copyfiles' and 'eclasses' copy logic:
			
			ov = foundation.overlays[repo_dict["name"]]
			
			if "copyfiles" in ov and len(ov["copyfiles"]):
				# since we copied over some ebuilds, we also want to make sure we copy over things like masks, etc:
				steps += [mu.SyncFiles(repo_dict["repo"].root, ov["copyfiles"])]
			if "eclasses" in ov:
				# we have eclasses to copy over, too:
				ec_files = {}
				for eclass in ov["eclasses"]:
					ecf = "/eclass/" + eclass + ".eclass"
					ec_files[ecf] = ecf
				steps += [mu.SyncFiles(repo_dict["repo"].root, ec_files)]
		copycount = cpm_logger.copycount
	
	# Phase 3: copy eclasses, licenses, profile info, and ebuild/eclass fixups from the kit-fixups repository. 
	
	# First, we are going to process the kit-fixups repository and look for ebuilds and eclasses to replace. Eclasses can be
	# overridden by using the following paths inside kit-fixups:
	
	# kit-fixups/eclass <--------------------- global eclasses, get installed to all kits unconditionally (overrides those above)
	# kit-fixups/<kit>/global/eclass <-------- global eclasses for a particular kit, goes in all branches (overrides those above)
	# kit-fixups/<kit>/global/profiles <------ global profile info for a particular kit, goes in all branches (overrides those above)
	# kit-fixups/<kit>/<branch>/eclass <------ eclasses to install in just a specific branch of a specific kit (overrides those above)
	# kit-fixups/<kit>/<branch>/profiles <---- profile info to install in just a specific branch of a specific kit (overrides those above)
	
	# Note that profile repo_name and categories files are excluded from any copying.
	
	# Ebuilds can be installed to kits by putting them in the following location(s):
	
	# kit-fixups/<kit>/global/cat/pkg <------- install cat/pkg into all branches of a particular kit
	# kit-fixups/<kit>/<branch>/cat/pkg <----- install cat/pkg into a particular branch of a kit
	
	# Remember that at this point, we may be missing a lot of eclasses and licenses from Gentoo. We will then perform a final sweep
	# of all catpkgs in the dest_kit and auto-detect missing eclasses from Gentoo and copy them to our dest_kit. Remember that if you
	# need a custom eclass from a third-party overlay, you will need to specify it in the overlay's overlays["ov_name"]["eclasses"]
	# list. Or alternatively you can copy the eclasses you need to kit-fixups and maintain them there :)
	
	steps = []
	
	# Here is the core logic that copies all the fix-ups from kit-fixups (eclasses and ebuilds) into place:
	
	if os.path.exists(fixup_repo.root + "/eclass"):
		steps += [mu.InsertEclasses(fixup_repo, select="all", skip=None)]
	if kit_dict["branch"] == "master":
		fixup_dirs = ["global", "master"]
	else:
		fixup_dirs = ["global", "curated", kit_dict["branch"]]
	for fixup_dir in fixup_dirs:
		fixup_path = kit_dict['name'] + "/" + fixup_dir
		if os.path.exists(fixup_repo.root + "/" + fixup_path):
			if os.path.exists(fixup_repo.root + "/" + fixup_path + "/eclass"):
				steps += [
					mu.InsertFilesFromSubdir(fixup_repo, "eclass", ".eclass", select="all", skip=None, src_offset=fixup_path)
				]
			if os.path.exists(fixup_repo.root + "/" + fixup_path + "/licenses"):
				steps += [
					mu.InsertFilesFromSubdir(fixup_repo, "licenses", None, select="all", skip=None, src_offset=fixup_path)
				]
			if os.path.exists(fixup_repo.root + "/" + fixup_path + "/profiles"):
				steps += [
					mu.InsertFilesFromSubdir(fixup_repo, "profiles", None, select="all", skip=["repo_name", "categories"], src_offset=fixup_path)
				]
			# copy appropriate kit readme into place:
			readme_path = fixup_path + "/README.rst"
			if os.path.exists(fixup_repo.root + "/" + readme_path):
				steps += [
					mu.SyncFiles(fixup_repo.root, {
						readme_path: "README.rst"
					})
				]
			
			# We now add a step to insert the fixups, and we want to record them as being copied so successive kits
			# don't get this particular catpkg. Assume we may not have all these catpkgs listed in our package-set
			# file...
			
			steps += [
				mu.InsertEbuilds(fixup_repo, ebuildloc=fixup_path, select="all", skip=None, replace=True,
								 cpm_logger=cpm_logger, is_fixup=True)
			]
	await tree.run(steps)
	
	# Now we want to perform a scan of any eclasses in the Gentoo repo that we need to copy over to our dest_kit so that it contains all
	# eclasses and licenses it needs within itself, without having to reference any in the Gentoo repo.
	
	copy_steps = []
	
	# For eclasses we perform a much more conservative scan. We will only scour missing eclasses from gentoo-staging, not
	# eclasses. If you need a special eclass, you need to specify it in the eclasses list for the overlay explicitly.
	
	await tree.run(copy_steps)
	copy_steps = []
	
	# copy all available licenses that have not been copied in fixups from gentoo-staging over to the kit.
	# We will remove any unused licenses below...
	
	copy_steps += [mu.InsertLicenses(gentoo_staging, select=mu.simpleGetAllLicenses(tree, gentoo_staging))]
	await tree.run(copy_steps)
	
	# Phase 4: finalize and commit
	
	# remove unused licenses...
	used_licenses = await mu.getAllLicenses(tree)
	to_remove = []
	for license in os.listdir(tree.root + "/licenses"):
		if license not in used_licenses["dest_kit"]:
			to_remove.append(tree.root + "/licenses/" + license)
	for file in to_remove:
		os.unlink(file)
	
	post_steps += [
		mu.ELTSymlinkWorkaround(),
		mu.CreateCategories(gentoo_staging),
		# multi-plex this and store in different locations so that different selections can be made based on which python-kit is enabled.
		# python-kit itself only needs one set which will be enabled by default.
	]
	
	if kit_dict["name"] == "python_kit":
		# on the python-kit itself, we only need settings for ourselves (not other branches)
		python_settings = foundation.python_kit_settings[kit_dict["name"]]
	else:
		# all other kits -- generate multiple settings, depending on what version of python-kit is active -- epro will select the right one for us.
		python_settings = foundation.python_kit_settings
	
	for branch, py_settings in python_settings.items():
		post_steps += [mu.GenPythonUse(py_settings, "funtoo/kits/python-kit/%s" % branch)]
	
	post_steps += [
		mu.Minify(),
		mu.GenUseLocalDesc(),
		mu.GenCache(cache_dir="/var/cache/edb/%s-%s" % (kit_dict['name'], kit_dict['branch'])),
	]
	
	post_steps += [
		mu.CatPkgScan(now=now, engine=async_engine)
	]
	
	await tree.run(post_steps)
	await tree.gitCommit(message="updates", push=push)
	return tree.head()


def generate_kit_metadata(foundation, release, meta_repo, output_sha1s):
	"""
	Generates the metadata in /var/git/meta-repo/metadata/...
	:param release: the release string, like "1.3-release".
	:param meta_repo: the meta-repo GitTree.
	:return: None.
	"""
	
	global KitRatingString
	global KitStabilityRating
	
	if not os.path.exists(meta_repo.root + "/metadata"):
		os.makedirs(meta_repo.root + "/metadata")
	
	with open(meta_repo.root + "/metadata/kit-sha1.json", "w") as a:
		a.write(json.dumps(output_sha1s, sort_keys=True, indent=4, ensure_ascii=False))
	
	outf = meta_repo.root + "/metadata/kit-info.json"

	with open(outf, 'w') as a:
		k_info = {}
		out = []
		out_settings = defaultdict(dict)
		for kit_dict in foundation.kit_groups[release]:
			kit_name = kit_dict["name"]
			if kit_name not in out:
				out.append(kit_name)
			# specific keywords that can be set for each branch to identify its current quality level
			if 'stability' not in out_settings[kit_name]:
				out_settings[kit_name]['stability'] = {}
			out_settings[kit_name]['stability'][kit_dict["branch"]] = KitRatingString(kit_dict["stability"])
		k_info["kit_order"] = out
		k_info["kit_settings"] = out_settings
		
		# auto-generate release-defs. We used to define them manually in foundation:

		kit_name_set = OrderedDict()
		for kit_dict in foundation.kit_groups[release]:
			kit_name_set[kit_dict["name"]] = True

		rdefs = {}
		for kit_name in kit_name_set.keys():
			rdefs[kit_name] = []
			for def_kit in filter(lambda x: x["name"] == kit_name and x["stability"] not in [KitStabilityRating.DEPRECATED], foundation.kit_groups[release]):
				rdefs[kit_name].append(def_kit["branch"])
	
		if release in [ "1.2-release"]:
			# metadata format version 1:
			k_info["release_defs"] = { "1.2" : rdefs }
		else:
			# newer metadata format version 10: drop release version dict:
			k_info["release_defs"] = rdefs
		k_info["release_info"] = getattr(foundation, "release_info", None)
		a.write(json.dumps(k_info, sort_keys=True, indent=4, ensure_ascii=False))
	
	with open(meta_repo.root + "/metadata/version.json", "w") as a:
		a.write(json.dumps(foundation.metadata_version_info[release], sort_keys=True, indent=4, ensure_ascii=False))

class KitQualityError(Exception):
	
	pass

async def kit_qa_check(foundation):

	# Make sure we don't redefine the same kit branch -- it's bad.
	
	all_kit_branches = defaultdict(dict)
	independent_kits = defaultdict(set)
	master_independent_kits = defaultdict(set)
	for release, kit_list in foundation.kit_groups.items():
		for kit in kit_list:
			if release != "independently-maintained":
				if "generate" in kit and kit["generate"] is False:
					independent_kits[kit["name"]].add(kit["branch"])
					continue
				kit_name = kit["name"]
				kit_branch = kit["branch"]
				if kit_branch in all_kit_branches[kit_name]:
					raise KitQualityError("Kit %s branch %s is defined multiple times. Exiting." % (kit_name, kit_branch))
				if kit_name in independent_kits:
					raise KitQualityError("Kit %s is already tagged as independently-maintained but auto-generated entry exists. This is not allowed. Exiting." % kit_name)
				all_kit_branches[kit_name][kit_branch] = kit
			else:
				master_independent_kits[kit["name"]].add(kit["branch"])
	for kit_name, branch_set in independent_kits.items():
		if kit_name not in master_independent_kits:
			raise KitQualityError("Kit %s is defined as not auto-generated, so should be listed in 'independently-maintained' kit_groups. Exiting." % kit_name)
		undefined_set = branch_set - master_independent_kits[kit_name]
		if len(undefined_set):
			raise KitQualityError("Kit %s has the following branches that are not defined in 'independently-maintained' kit_groups section: %s. Exiting." % (kit_name, str(list(undefined_set))))
	return True


async def main_thread():
	# one global timestamp for each run of this tool -- for mysql db
	now = datetime.utcnow()
	
	fixup_repo = mu.GitTree("kit-fixups", config.branch("kit-fixups"), url=config.kit_fixups, root=config.source_trees + "/kit-fixups")
	await fixup_repo.initialize()
	meta_repo = mu.GitTree("meta-repo", config.branch("meta-repo"), url=config.base_url("meta-repo"), root=config.dest_trees + "/meta-repo")
	await meta_repo.initialize()
	
	sys.path.insert(0, fixup_repo.root + "/modules")
	from fixups.foundations import KitFoundation, KitRatingString, KitStabilityRating
	
	foundation = KitFoundation(config)
	# Now that we imported, set these guys to be globals:
	globals()['KitRatingString'] = KitRatingString
	globals()['KitStabilityRating'] = KitStabilityRating
	
	if len(sys.argv) < 2 or sys.argv[1] not in ["push", "nopush"]:
		print("Please specify push or nopush as an argument.")
		sys.exit(1)
	else:
		push = True if "push" in sys.argv else False
	
	try:
		await kit_qa_check(foundation)
	except KitQualityError as e:
		print("Kit QA Error detected:", str(e))
		sys.exit(1)

	num_threads = 40
	async_engine = None
	
	if "db" in sys.argv:
		async_engine = AsyncMergeAllKits(num_threads=num_threads)
		async_engine.start_threads(enable_workers=True if num_threads != 0 else False)
	
	for release in foundation.kit_groups.keys():
		
		cpm_logger = mu.CatPkgMatchLogger(log_xml=push)
		if not release.endswith("-release"):
			continue
		
		target_branch = "master" if release == "1.2-release" else release
		await meta_repo.gitCheckout(target_branch)
		
		output_sha1s = {}
		prev_kit_dict = None
		
		for kit_dict in foundation.kit_groups[release]:
			print("Regenerating kit ", kit_dict)
			head = await updateKit(foundation, release, async_engine, kit_dict, prev_kit_dict, cpm_logger, create=not push, push=push, now=now, fixup_repo=fixup_repo)
			kit_name = kit_dict["name"]
			if kit_name not in output_sha1s:
				output_sha1s[kit_name] = {}
			output_sha1s[kit_name][kit_dict["branch"]] = head
			prev_kit_dict = kit_dict
		generate_kit_metadata(foundation, release, meta_repo, output_sha1s)
		
		await meta_repo.gitCommit(message="kit updates", push=False)
	
	if push is True:
		print("Pushing meta-repo...")
		await meta_repo.gitPush()
	
	if async_engine is not None:
		await async_engine.wait_for_workers_to_finish()


if __name__ == "__main__":
	
	loop = asyncio.get_event_loop()
	loop.run_until_complete(main_thread())
	sys.exit(0)

# vim: ts=4 sw=4 noet tw=140
