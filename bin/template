#!/usr/bin/python3

import os
import sys
sys.path.insert(0, os.path.normpath(os.path.join(os.path.realpath(__file__), "../../modules")))

from jinja2 import Template
from glob import glob
import multiprocessing
from concurrent.futures import ThreadPoolExecutor
import asyncio
import os
from merge.merge_utils import runShell
import itertools
from collections import defaultdict
from enum import Enum

# https://chrispenner.ca/posts/python-tail-recursion
class Recurse(Exception):
	def __init__(self, *args, **kwargs):
		self.args = args
		self.kwargs = kwargs


def recurse(*args, **kwargs):
	raise Recurse(*args, **kwargs)


def tail_recursive(f):
	def decorated(*args, **kwargs):
		while True:
			try:
				return f(*args, **kwargs)
			except Recurse as r:
				args = r.args
				kwargs = r.kwargs
				continue
	return decorated

class ExecutionClass(Enum):
	THREAD = "thread"
	ASYNC = "async"

class ExecutionPipeline:
	
	def __init__(self, executor=None, max_workers=None):
		self.cpu_bound_executor = executor if executor is not None else ThreadPoolExecutor(max_workers=multiprocessing.cpu_count() if max_workers is None else max_workers)
		self.loop = asyncio.get_event_loop()
		self.pipeline = []
		# where to store futures:
		self.awaitables = []
	
	def append(self, step, exec_class=ExecutionClass.ASYNC):
		self.pipeline.append((step, exec_class))

	def run_async(self, corofn, *args):
		loop = asyncio.new_event_loop()
		try:
			future = corofn(*args)
			asyncio.set_event_loop(loop)
			return loop.run_until_complete(future)
		finally:
			loop.close()
			
	def run(self):
		self.awaitables += [
			self.loop.run_in_executor(self.cpu_bound_executor, self.run_async, self.pipeline[0])
		]
		while True:
			done, pending = asyncio.wait(self.awaitables, return_when=asyncio.FIRST_COMPLETED)
			for d in done:
				result = d.result()
				next_pos = result.pipeline_position + 1
				if next_pos >= len(self.pipeline):
					yield result
				else:
					self.awaitables += [
						self.loop.run_in_executor(self.cpu_bound_executor, self.run_async, self.pipeline[result.pipeline_position+1], result)
					]
			
			
			
		#self.tasks.append(asyncio.ensure_future(foo(pipeline_position=0)))

class ExecutionResult:
	
	def __init__(self, pipeline_position):
		self.pipeline_position = pipeline_position
	
class ExecutionStep:
	
	async def start(self, pipeline_position):
		return pipeline_position
	

class Tree:
	futures = []
	
	async def run(self, steps, result=None):
		# in a loop --
			#add futures to list
		
		self.futures += [ "blas", "basd" ]
		while True:
			self.loop.run_until_completed(asyncio.wait(self.tasks, return_when=asyncio.FIRST_COMPLETED))
			for task in done:
				result = task.result()
				
			# process done results... create more futures....
		
		
		
		
		
		# generate futures for current step.....
		
		# then recurse with remaining steps
		# this should work ok for one-to-one stuff, haven't figured out one-to-many
		
		for step in steps:
			if step is not None:
				print("Running step", step.__class__.__name__, step)
				await step.run(self)
		
		futures = [
			self.loop.run_in_executor(self.cpu_bound_executor, self.run_async, self.worker_async, all_meta_atoms, meta_pkg_ebuild_path)
			for meta_pkg_ebuild_path in all_meta_pkg_ebuilds
		]
		meta_mappings = defaultdict(set)
		for future in asyncio.as_completed(futures):
			new_meta_mappings = await future
			for key, new_set in new_meta_mappings.items():
				meta_mappings[key] |= new_set




class EbuildTemplate:

	template_text = None

	def __init__(self):
		
		self.template = Template(self.template_text)
	
	def render(self, **kwargs):
		return self.template.render(**kwargs)
	

class EbuildGenerator:
	
	template = Template("""# Distributed under the terms of the GNU General Public License v2
EAPI=6

inherit multilib-minimal

DESCRIPTION="X.Org Protocol ${proto} package stub (provided by {{master_cpv}})."

KEYWORDS="*"

SLOT="0"

RDEPEND="|| ({% for meta_atom in all_meta_atoms %}
	={{meta_atom}}
{%- endfor %}
)"
DEPEND="${RDEPEND}"

S="${WORKDIR}"

multilib_src_configure() { return 0; }
src_configure() { return 0; }
multilib_src_compile() { return 0; }
src_compile() { return 0; }
multilib_src_install() { return 0; }
src_install() { return 0; }

""")
	
	def __init__(self, fixup_subpath, executor=None, max_workers=None):
		self.fixup_subpath = fixup_subpath
		self.cpu_bound_executor = executor if executor is not None else ThreadPoolExecutor(max_workers=multiprocessing.cpu_count() if max_workers is None else max_workers)
		self.loop = asyncio.get_event_loop()
	
	def get_pkgs_from_meson(self, master_cpv, fn, prefix="pcs"):
		capture = False
		
		with open(fn, "r") as f:
			lines = f.readlines()
			for line in lines:
				ls = line.strip()
				if ls.startswith("%s = [" % prefix):
					capture = True
				elif capture is True:
					if ls == "]":
						break
					else:
						ls = ls.lstrip("[").rstrip("],").split(",")
						pkg = ls[0].strip().strip("'")
						ver = ls[1].strip().strip("'")
						yield master_cpv, pkg, ver
	
	def run_async(self, corofn, *args):
		loop = asyncio.new_event_loop()
		try:
			future = corofn(*args)
			asyncio.set_event_loop(loop)
			return loop.run_until_complete(future)
		finally:
			loop.close()
	
	def get_catpkg_from_ebuild_path(self, path):
		spl = path.rstrip(".ebuild").split("/")
		return spl[-3] + "/" + spl[-1]
	
	async def worker_async(self, all_meta_atoms, meta_pkg_ebuild_path):
		sdata = meta_pkg_ebuild_path.rstrip(".ebuild").split("/")
		master_cpv = sdata[-3] + "/" + sdata[-1]
		await runShell("(cd %s; ebuild %s clean unpack)" % (os.path.dirname(meta_pkg_ebuild_path), os.path.basename(meta_pkg_ebuild_path)), abort_on_failure=True)
		meson_file = os.path.expanduser("~portage/%s/work/xorg*proto-*/meson.build" % master_cpv)
		meson_file = glob(meson_file)
		if len(meson_file) != 1 or not os.path.exists(meson_file[0]):
			print("File not found:", meson_file)
		else:
			meson_file = meson_file[0]
		meta_mappings = defaultdict(set)
		for master_cpv, pkg, ver in itertools.chain(self.get_pkgs_from_meson(master_cpv, meson_file), self.get_pkgs_from_meson(master_cpv, meson_file, "legacy_pcs")):
			meta_mappings[(pkg, ver)].add(master_cpv)
		await runShell("(cd %s; ebuild %s clean)" % (os.path.dirname(meta_pkg_ebuild_path), os.path.basename(meta_pkg_ebuild_path)), abort_on_failure=True)
		return meta_mappings
		
	async def _run(self):
		all_meta_pkg_ebuilds = list(glob(self.fixup_subpath + "/x11-base/xorg-proto/xorg-proto-*.ebuild"))
		all_meta_atoms = list(map(self.get_catpkg_from_ebuild_path, all_meta_pkg_ebuilds))
		futures =[
			self.loop.run_in_executor(self.cpu_bound_executor, self.run_async, self.worker_async, all_meta_atoms, meta_pkg_ebuild_path)
			for meta_pkg_ebuild_path in all_meta_pkg_ebuilds
		]
		meta_mappings = defaultdict(set)
		for future in asyncio.as_completed(futures):
			new_meta_mappings = await future
			for key, new_set in new_meta_mappings.items():
				meta_mappings[key] |= new_set
		
		for pv_key, all_meta_atoms in meta_mappings.items():
			pkg, ver = pv_key
			all_meta_atoms = sorted(list(all_meta_atoms))
			output_ebuild = self.fixup_subpath + "/x11-proto/%s/%s-%s.ebuild" % ( pkg, pkg, ver )
			output_dir = os.path.dirname(output_ebuild)
			if not os.path.exists(output_dir):
				os.makedirs(output_dir)
			with open(output_ebuild, "w") as f:
				print('Generating %s...' % output_ebuild)
				f.write(self.template.render(all_meta_atoms=all_meta_atoms))
		
	def run(self):
		asyncio.get_child_watcher()
		self.loop.run_until_complete(self._run())
	
if __name__ == "__main__":
	eb = EbuildGenerator(fixup_subpath="/var/src/kit-fixups/core-gl-kit/1.3-release")
	eb.run()
	
	
